# -*- coding: utf-8 -*-
"""EmotionalInferenceWithDeepSeekR1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GYD2tBle_YJ9JKAZrur_RKAaX48jjRDn
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # NUCLEAR OPTION - Reset everything
# !pip uninstall transformers unsloth unsloth_zoo -y
# !pip install --upgrade transformers
# !pip install accelerate bitsandbytes

from huggingface_hub import login
from google.colab import userdata

hf_token = userdata.get('HuggingFaceToken')
login(token=hf_token)

import wandb

wb_token = userdata.get("WabKey")

wandb.login(key=wb_token)
run = wandb.init(
    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Poetry Dataset',
    job_type="training",
    anonymous="allow"
)

# Trying the original DeepSeek model, not Unsloth's version
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True,
    token=hf_token
)

"""DeepSeek-R1-Distill-Llama-8B is a smaller, more efficient model that is a distilled version of the larger DeepSeek-R1 model. It uses the Llama3.1-8B architecture as a base and has been fine-tuned with knowledge from the DeepSeek-R1 model to improve reasoning, math, and coding performance while reducing computational requirements. Therefore, the key difference is that the distilled model sacrifices some of the raw capabilities of the larger model for increased efficiency and easier deployment"""

from google.colab import files
import pandas as pd

uploaded = files.upload()

data = pd.read_csv("isear_clean_drop.csv")
data = data.rename(columns={'SIT': 'SITUATION', 'Field1': 'EMOTION'})
data.head()

prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully and provide your reasoning inside <think> tags.

### Instruction:
You are an emotion analysis expert with advanced knowledge in emotional interpretation and emotional nuance.
Analyze the emotional undertones of the following sentence and determine the dominant emotion using ONLY this list:
Anger, Disgust, Fear, Guilt, Joy, Sadness, Shame.

IMPORTANT:
After the reasoning, output EXACTLY on a new line:
FINAL_LABEL: <one word from the list above>
Do NOT output anything else on that line.

### Text:
{}

### Response:
<think>{}</think>

FINAL_LABEL:"""

"""HardCoding Situation Text In to Test Prompt"""

text = "At a gathering I found myself involuntarily sitting next to two  people who expressed opinions that I considered very low and  discriminating."
prompt = prompt_style.format(text, "")

inputs = tokenizer([prompt], return_tensors="pt", padding=True, truncation=True, max_length=1024)
inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}

gen = model.generate(
    input_ids = inputs["input_ids"],
    attention_mask = inputs["attention_mask"],
    max_new_tokens = 1200,
    use_cache = True,
)

raw = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]
print("RAW OUTPUT:\n", raw, "\n")# Test Poem from reddit database

import re
from tqdm import tqdm

EMOTIONS = ["Anger", "Disgust", "Fear", "Guilt", "Joy", "Sadness", "Shame"]

def extract_final_label(raw_text):
    """
    Extract the emotion ONLY from the model’s FINAL_LABEL line.
    This avoids false matches from the prompt.
    """
    match = re.search(r"FINAL_LABEL:\s*(.*)$", raw_text, flags=re.I)
    if not match:
        return None

    candidate = match.group(1).strip().split()[0]  # first token after FINAL_LABEL:

    # Normalize + validate
    for e in EMOTIONS:
        if candidate.lower().strip(".,;:") == e.lower():
            return e
    return candidate.capitalize()


preds = []

print("Running inference on FIRST 20 rows...\n") #As test

for i, row in tqdm(data.iloc[:20].iterrows(), total=20):
    text = row["SITUATION"]

    prompt = prompt_style.format(text, "")

    inputs = tokenizer(
        [prompt],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024
    )
    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}

    gen = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=128,
        do_sample=False,
        temperature=0.0,
        use_cache=True,
    )

    raw = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]

    pred = extract_final_label(raw)
    preds.append(pred)

data.loc[:19, "predicted_emotion"] = preds

print("\n=== FIRST 20 PREDICTIONS ===")
print(data.loc[:19, ["SITUATION", "EMOTION", "predicted_emotion"]])

"""Output Of First 20 cases in a Structured DataSet"""

data = data[["ID", "SITUATION", "EMOTION", "predicted_emotion"]]
data.head(10)

"""Now To Test Models Inference On All 5724 Cases"""

import transformers
transformers.logging.set_verbosity_error()
import re
from tqdm import tqdm

EMOTIONS = ["Anger", "Disgust", "Fear", "Guilt", "Joy", "Sadness", "Shame"]

def extract_final_label(raw_text):
    """
    Extract emotion ONLY from the FINAL_LABEL line.
    This avoids matching words in the prompt, withous this matching is inaccurate and repetitive.
    """
    match = re.search(r"FINAL_LABEL:\s*(.*)$", raw_text, flags=re.I)
    if not match:
        return None

    candidate = match.group(1).strip().split()[0]


    for e in EMOTIONS:
        if candidate.lower().strip(".,;:") == e.lower():
            return e

    return candidate.capitalize()


preds = []

print(f"Running inference on ALL {len(data)} rows...\n")

for i, row in tqdm(data.iterrows(), total=len(data)):
    text = row["SITUATION"]

    prompt = prompt_style.format(text, "")

    inputs = tokenizer(
        [prompt],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024
    )
    inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}

    gen = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=128,
        do_sample=False,
        temperature=0.0,
        use_cache=True,
    )

    raw = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]

    pred = extract_final_label(raw)
    preds.append(pred)

data["predicted_emotion"] = preds

# I Reordered columns (ID, SITUATION, EMOTION, predicted_emotion) for better readability.
data = data[["ID", "SITUATION", "EMOTION", "predicted_emotion"]]
data.to_csv("emotion_predictions_full.csv", index=False)

print("\n✅ DONE! Saved file: emotion_predictions_full.csv")

data["predicted_emotion"] = data["predicted_emotion"].str.lower()
data.head(50)
data.to_csv("emotion_predictions_full.csv", index=False)
from google.colab import files
files.download("emotion_predictions_full.csv") # Just downloaded to desktop

"""Testing Module Inference"""

data["predicted_emotion"] = data["predicted_emotion"].fillna("unknown")

TARGET_EMOTIONS = ["anger", "disgust", "fear", "guilt", "joy", "sadness", "shame"]
clean = data[data["EMOTION"].isin(TARGET_EMOTIONS)].copy()
clean = clean[clean["predicted_emotion"].notna()]

from sklearn.metrics import classification_report, accuracy_score

y_true = clean["EMOTION"]
y_pred = clean["predicted_emotion"]

print("Accuracy:", accuracy_score(y_true, y_pred))

print(classification_report(
    y_true,
    y_pred,
    labels=TARGET_EMOTIONS,
    zero_division=0
))

"""This script evaluates the zero-shot emotion-classification performance of the DeepSeek R1 Distill-Llama-8B model on the ISEAR dataset. It loads each situation from the dataset, formats it using a custom prompt, runs it through the model, extracts the model’s final one-word emotion output, and appends the result as a new column (predicted_emotion). The code then saves the full dataframe and allows later calculation of accuracy and F1-scores. No fine-tuning is performed here — the script simply tests how well the base model can infer emotions from text."""